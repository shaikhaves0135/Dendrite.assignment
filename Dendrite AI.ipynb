{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc395d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from sklearn.impute import SimpleImputer\n",
    "file = open(\"case study.txt\",\"r\")\n",
    "df = pd.read_csv(\"C://Users/HP/OneDrive/Desktop/Screening Test - DS/iris.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c73b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data = file.read()\n",
    "dict1=json.loads(data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2db80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_name': 'test',\n",
       " 'session_description': 'test',\n",
       " 'design_state_data': {'session_info': {'project_id': '1',\n",
       "   'experiment_id': 'kkkk-11',\n",
       "   'dataset': 'iris_modified.csv',\n",
       "   'session_name': 'test',\n",
       "   'session_description': 'test'},\n",
       "  'target': {'prediction_type': 'Regression',\n",
       "   'target': 'petal_width',\n",
       "   'type': 'regression',\n",
       "   'partitioning': True},\n",
       "  'train': {'policy': 'Split the dataset',\n",
       "   'time_variable': 'sepal_length',\n",
       "   'sampling_method': 'No sampling(whole data)',\n",
       "   'split': 'Randomly',\n",
       "   'k_fold': False,\n",
       "   'train_ratio': 0,\n",
       "   'random_seed': 0},\n",
       "  'metrics': {'optomize_model_hyperparameters_for': 'AUC',\n",
       "   'optimize_threshold_for': 'F1 Score',\n",
       "   'compute_lift_at': 0,\n",
       "   'cost_matrix_gain_for_true_prediction_true_result': 1,\n",
       "   'cost_matrix_gain_for_true_prediction_false_result': 0,\n",
       "   'cost_matrix_gain_for_false_prediction_true_result': 0,\n",
       "   'cost_matrix_gain_for_false_prediction_false_result': 0},\n",
       "  'feature_handling': {'sepal_length': {'feature_name': 'sepal_length',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'Average of values',\n",
       "     'impute_value': 0}},\n",
       "   'sepal_width': {'feature_name': 'sepal_width',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'custom',\n",
       "     'impute_value': -1}},\n",
       "   'petal_length': {'feature_name': 'petal_length',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'Average of values',\n",
       "     'impute_value': 0}},\n",
       "   'petal_width': {'feature_name': 'petal_width',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'numerical',\n",
       "    'feature_details': {'numerical_handling': 'Keep as regular numerical feature',\n",
       "     'rescaling': 'No rescaling',\n",
       "     'make_derived_feats': False,\n",
       "     'missing_values': 'Impute',\n",
       "     'impute_with': 'custom',\n",
       "     'impute_value': -2}},\n",
       "   'species': {'feature_name': 'species',\n",
       "    'is_selected': True,\n",
       "    'feature_variable_type': 'text',\n",
       "    'feature_details': {'text_handling': 'Tokenize and hash',\n",
       "     'hash_columns': 0}}},\n",
       "  'feature_generation': {'linear_interactions': [['petal_length',\n",
       "     'sepal_width']],\n",
       "   'linear_scalar_type': 'robust',\n",
       "   'polynomial_interactions': ['petal_length/sepal_width',\n",
       "    'petal_width/species'],\n",
       "   'explicit_pairwise_interactions': ['sepal_width/sepal_length',\n",
       "    'petal_width/sepal_length']},\n",
       "  'feature_reduction': {'feature_reduction_method': 'Tree-based',\n",
       "   'num_of_features_to_keep': '4',\n",
       "   'num_of_trees': '5',\n",
       "   'depth_of_trees': '6'},\n",
       "  'hyperparameters': {'stratergy': 'Grid Search',\n",
       "   'shuffle_grid': True,\n",
       "   'random_state': 1,\n",
       "   'max_iterations': 2,\n",
       "   'max_search_time': 3,\n",
       "   'parallelism': 5,\n",
       "   'cross_validation_stratergy': 'Time-based K-fold(with overlap)',\n",
       "   'num_of_folds': 6,\n",
       "   'split_ratio': 0,\n",
       "   'stratified': True},\n",
       "  'weighting_stratergy': {'weighting_stratergy_method': 'Sample weights',\n",
       "   'weighting_stratergy_weight_variable': 'petal_length'},\n",
       "  'probability_calibration': {'probability_calibration_method': 'Sigmoid - Platt Scaling'},\n",
       "  'algorithms': {'RandomForestClassifier': {'model_name': 'Random Forest Classifier',\n",
       "    'is_selected': False,\n",
       "    'min_trees': 10,\n",
       "    'max_trees': 30,\n",
       "    'feature_sampling_statergy': 'Default',\n",
       "    'min_depth': 20,\n",
       "    'max_depth': 30,\n",
       "    'min_samples_per_leaf_min_value': 5,\n",
       "    'min_samples_per_leaf_max_value': 50,\n",
       "    'parallelism': 0},\n",
       "   'RandomForestRegressor': {'model_name': 'Random Forest Regressor',\n",
       "    'is_selected': True,\n",
       "    'min_trees': 10,\n",
       "    'max_trees': 20,\n",
       "    'feature_sampling_statergy': 'Default',\n",
       "    'min_depth': 20,\n",
       "    'max_depth': 25,\n",
       "    'min_samples_per_leaf_min_value': 5,\n",
       "    'min_samples_per_leaf_max_value': 10,\n",
       "    'parallelism': 0},\n",
       "   'GBTClassifier': {'model_name': 'Gradient Boosted Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_BoostingStages': [67, 89],\n",
       "    'feature_sampling_statergy': 'Fixed number',\n",
       "    'learningRate': [],\n",
       "    'use_deviance': True,\n",
       "    'use_exponential': False,\n",
       "    'fixed_number': 22,\n",
       "    'min_subsample': 1,\n",
       "    'max_subsample': 2,\n",
       "    'min_stepsize': 0.1,\n",
       "    'max_stepsize': 0.5,\n",
       "    'min_iter': 20,\n",
       "    'max_iter': 40,\n",
       "    'min_depth': 5,\n",
       "    'max_depth': 7},\n",
       "   'GBTRegressor': {'model_name': 'Gradient Boosted Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_BoostingStages': [67, 89],\n",
       "    'feature_sampling_statergy': 'Fixed number',\n",
       "    'use_deviance': True,\n",
       "    'use_exponential': False,\n",
       "    'fixed_number': 22,\n",
       "    'min_subsample': 1,\n",
       "    'max_subsample': 2,\n",
       "    'min_stepsize': 0.1,\n",
       "    'max_stepsize': 0.5,\n",
       "    'min_iter': 20,\n",
       "    'max_iter': 40,\n",
       "    'min_depth': 5,\n",
       "    'max_depth': 7},\n",
       "   'LinearRegression': {'model_name': 'LinearRegression',\n",
       "    'is_selected': False,\n",
       "    'parallelism': 2,\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'LogisticRegression': {'model_name': 'LogisticRegression',\n",
       "    'is_selected': False,\n",
       "    'parallelism': 2,\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'RidgeRegression': {'model_name': 'RidgeRegression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8},\n",
       "   'LassoRegression': {'model_name': 'Lasso Regression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8},\n",
       "   'ElasticNetRegression': {'model_name': 'Lasso Regression',\n",
       "    'is_selected': False,\n",
       "    'regularization_term': 'Specify values to test',\n",
       "    'min_iter': 30,\n",
       "    'max_iter': 50,\n",
       "    'min_regparam': 0.5,\n",
       "    'max_regparam': 0.8,\n",
       "    'min_elasticnet': 0.5,\n",
       "    'max_elasticnet': 0.8},\n",
       "   'xg_boost': {'model_name': 'XG Boost',\n",
       "    'is_selected': False,\n",
       "    'use_gradient_boosted_tree': True,\n",
       "    'dart': True,\n",
       "    'tree_method': '',\n",
       "    'random_state': 0,\n",
       "    'max_num_of_trees': 0,\n",
       "    'early_stopping': True,\n",
       "    'early_stopping_rounds': 2,\n",
       "    'max_depth_of_tree': [56, 89],\n",
       "    'learningRate': [89, 76],\n",
       "    'l1_regularization': [77],\n",
       "    'l2_regularization': [78],\n",
       "    'gamma': [68],\n",
       "    'min_child_weight': [67],\n",
       "    'sub_sample': [67],\n",
       "    'col_sample_by_tree': [67],\n",
       "    'replace_missing_values': False,\n",
       "    'parallelism': 0},\n",
       "   'DecisionTreeRegressor': {'model_name': 'Decision Tree',\n",
       "    'is_selected': False,\n",
       "    'min_depth': 4,\n",
       "    'max_depth': 7,\n",
       "    'use_gini': False,\n",
       "    'use_entropy': True,\n",
       "    'min_samples_per_leaf': [12, 6],\n",
       "    'use_best': True,\n",
       "    'use_random': True},\n",
       "   'DecisionTreeClassifier': {'model_name': 'Decision Tree',\n",
       "    'is_selected': False,\n",
       "    'min_depth': 4,\n",
       "    'max_depth': 7,\n",
       "    'use_gini': False,\n",
       "    'use_entropy': True,\n",
       "    'min_samples_per_leaf': [12, 6],\n",
       "    'use_best': True,\n",
       "    'use_random': True},\n",
       "   'SVM': {'model_name': 'Support Vector Machine',\n",
       "    'is_selected': False,\n",
       "    'linear_kernel': True,\n",
       "    'rep_kernel': True,\n",
       "    'polynomial_kernel': True,\n",
       "    'sigmoid_kernel': True,\n",
       "    'c_value': [566, 79],\n",
       "    'auto': True,\n",
       "    'scale': True,\n",
       "    'custom_gamma_values': True,\n",
       "    'tolerance': 7,\n",
       "    'max_iterations': 7},\n",
       "   'SGD': {'model_name': 'Stochastic Gradient Descent',\n",
       "    'is_selected': False,\n",
       "    'use_logistics': True,\n",
       "    'use_modified_hubber_loss': False,\n",
       "    'max_iterations': False,\n",
       "    'tolerance': 56,\n",
       "    'use_l1_regularization': 'on',\n",
       "    'use_l2_regularization': 'on',\n",
       "    'use_elastic_net_regularization': True,\n",
       "    'alpha_value': [79, 56],\n",
       "    'parallelism': 1},\n",
       "   'KNN': {'model_name': 'KNN',\n",
       "    'is_selected': False,\n",
       "    'k_value': [78],\n",
       "    'distance_weighting': True,\n",
       "    'neighbour_finding_algorithm': 'Automatic',\n",
       "    'random_state': 0,\n",
       "    'p_value': 0},\n",
       "   'extra_random_trees': {'model_name': 'Extra Random Trees',\n",
       "    'is_selected': False,\n",
       "    'num_of_trees': [45, 489],\n",
       "    'feature_sampling_statergy': 'Square root and Logarithm',\n",
       "    'max_depth': [12, 45],\n",
       "    'min_samples_per_leaf': [78, 56],\n",
       "    'parallelism': 3},\n",
       "   'neural_network': {'model_name': 'Neural Network',\n",
       "    'is_selected': False,\n",
       "    'hidden_layer_sizes': [67, 89],\n",
       "    'activation': '',\n",
       "    'alpha_value': 0,\n",
       "    'max_iterations': 0,\n",
       "    'convergence_tolerance': 0,\n",
       "    'early_stopping': True,\n",
       "    'solver': 'ADAM',\n",
       "    'shuffle_data': True,\n",
       "    'initial_learning_rate': 0,\n",
       "    'automatic_batching': True,\n",
       "    'beta_1': 0,\n",
       "    'beta_2': 0,\n",
       "    'epsilon': 0,\n",
       "    'power_t': 0,\n",
       "    'momentum': 0,\n",
       "    'use_nesterov_momentum': False}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69c350c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "train : 0.9508373303604417\n",
      "test : 0.9519107849759174\n",
      "r2_score : 0.9519107849759174\n",
      "----------------------------------------------------------------------------------------------------\n",
      "DecisionTreeRegressor\n",
      "train : 0.9999213932485223\n",
      "test : 0.8993365787914348\n",
      "r2_score : 0.8993365787914348\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RandomForestRegressor\n",
      "train : 0.9913220398976054\n",
      "test : 0.943182427616215\n",
      "r2_score : 0.943182427616215\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GBTRegressor\n",
      "train : 0.9887338900447172\n",
      "test : 0.941825522635523\n",
      "r2_score : 0.941825522635523\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/500\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.1475 - val_loss: 1.0169\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9141 - val_loss: 0.8200\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7200 - val_loss: 0.6511\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5607 - val_loss: 0.5083\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4205 - val_loss: 0.3928\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3204 - val_loss: 0.3013\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2451 - val_loss: 0.2318\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1923 - val_loss: 0.1817\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1543 - val_loss: 0.1480\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1312 - val_loss: 0.1257\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1190 - val_loss: 0.1115\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1111 - val_loss: 0.1024\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1054 - val_loss: 0.0959\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1021 - val_loss: 0.0908\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0986 - val_loss: 0.0871\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0954 - val_loss: 0.0840\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0921 - val_loss: 0.0813\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0891 - val_loss: 0.0789\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0861 - val_loss: 0.0767\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0833 - val_loss: 0.0747\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0807 - val_loss: 0.0730\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0786 - val_loss: 0.0714\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0762 - val_loss: 0.0699\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0746 - val_loss: 0.0683\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0728 - val_loss: 0.0667\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0707 - val_loss: 0.0653\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0692 - val_loss: 0.0640\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0676 - val_loss: 0.0626\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0661 - val_loss: 0.0610\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0645 - val_loss: 0.0595\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0631 - val_loss: 0.0582\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0616 - val_loss: 0.0572\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0603 - val_loss: 0.0561\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0592 - val_loss: 0.0551\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0542\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0575 - val_loss: 0.0534\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0565 - val_loss: 0.0527\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0556 - val_loss: 0.0521\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0551 - val_loss: 0.0516\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0542 - val_loss: 0.0508\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0535 - val_loss: 0.0503\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0530 - val_loss: 0.0498\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0523 - val_loss: 0.0492\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 0.0488\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0511 - val_loss: 0.0483\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0508 - val_loss: 0.0481\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0502 - val_loss: 0.0476\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.0469\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0493 - val_loss: 0.0464\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.0461\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0486 - val_loss: 0.0457\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0482 - val_loss: 0.0457\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.0459\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0474 - val_loss: 0.0461\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0469 - val_loss: 0.0463\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0465 - val_loss: 0.0465\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0462 - val_loss: 0.0465\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0459 - val_loss: 0.0465\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0457 - val_loss: 0.0464\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 0.0460\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0451 - val_loss: 0.0457\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.0456\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0444 - val_loss: 0.0452\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0441 - val_loss: 0.0451\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0438 - val_loss: 0.0449\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0436 - val_loss: 0.0450\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0433 - val_loss: 0.0448\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0432 - val_loss: 0.0448\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0428 - val_loss: 0.0446\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0425 - val_loss: 0.0444\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0424 - val_loss: 0.0446\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0420 - val_loss: 0.0443\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0419 - val_loss: 0.0437\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0417 - val_loss: 0.0436\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 0.0436\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0411 - val_loss: 0.0436\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0409 - val_loss: 0.0434\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0407 - val_loss: 0.0434\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0405 - val_loss: 0.0437\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0402 - val_loss: 0.0440\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0401 - val_loss: 0.0442\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0401 - val_loss: 0.0441\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0399 - val_loss: 0.0444\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0396 - val_loss: 0.0444\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0394 - val_loss: 0.0441\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0441\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0390 - val_loss: 0.0439\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0389 - val_loss: 0.0433\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0387 - val_loss: 0.0430\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0385 - val_loss: 0.0430\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0384 - val_loss: 0.0429\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0383 - val_loss: 0.0428\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0382 - val_loss: 0.0429\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0380 - val_loss: 0.0427\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0379 - val_loss: 0.0427\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0424\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.0424\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0374 - val_loss: 0.0424\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0373 - val_loss: 0.0425\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0373 - val_loss: 0.0432\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0370 - val_loss: 0.0431\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0369 - val_loss: 0.0428\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0369 - val_loss: 0.0425\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0367 - val_loss: 0.0425\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0368 - val_loss: 0.0423\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0365 - val_loss: 0.0424\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.0429\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0364 - val_loss: 0.0431\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0363 - val_loss: 0.0432\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0361 - val_loss: 0.0431\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0360 - val_loss: 0.0432\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.0432\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0431\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.0429\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0432\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0429\n",
      "Epoch 117/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0356 - val_loss: 0.0430\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0355 - val_loss: 0.0429\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0356 - val_loss: 0.0426\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 0.0427\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0353 - val_loss: 0.0427\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0352 - val_loss: 0.0429\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0352 - val_loss: 0.0430\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0429\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0428\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0351 - val_loss: 0.0426\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0350 - val_loss: 0.0426\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0348 - val_loss: 0.0428\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0349 - val_loss: 0.0426\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0348 - val_loss: 0.0424\n",
      "Epoch 130: early stopping\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "----------------------------------------------------------------------------------------------------\n",
      "r2_score: 0.9588663588030104\n"
     ]
    }
   ],
   "source": [
    "y = dict1[\"design_state_data\"][\"target\"][\"target\"]\n",
    "y = df[y]\n",
    "feature = []\n",
    "for feature1 in dict1[\"design_state_data\"][\"feature_handling\"]:\n",
    "    feature.append(feature1)\n",
    "missing_value = []\n",
    "for x in feature:\n",
    "    if (\"impute_with\" in dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"]) and (dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"]['impute_with'] == \"Average of values\"):\n",
    "        impute_value = dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"]['impute_value']\n",
    "        Average = df[x].mean()\n",
    "        df[x] = df[x].replace(int(impute_value),Average)\n",
    "    elif (\"impute_with\" in dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"]) and (dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"][\"impute_with\"] == \"custom\"):\n",
    "        impute_value = dict1[\"design_state_data\"][\"feature_handling\"][x][\"feature_details\"]['impute_value']\n",
    "        Custom = df[x].median()\n",
    "        df[x] = df[x].replace(int(impute_value),Custom)\n",
    "    else:\n",
    "        Custom = df[x].mode()\n",
    "        Custom1 = []\n",
    "        for i in Custom:\n",
    "            Custom1.append(i)\n",
    "        Custom2 = choice(Custom1)\n",
    "        df[x] = df[x].replace(np.nan,Custom2)\n",
    "x = df.drop(dict1[\"design_state_data\"][\"target\"][\"target\"],axis=1)\n",
    "col = []\n",
    "for i in x.columns:\n",
    "    col.append(i)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for i in col:\n",
    "    if (x[i]).dtype == \"object\":\n",
    "        LE = LabelEncoder()\n",
    "        x[i] = LE.fit_transform(x[i])\n",
    "    else:\n",
    "        continue\n",
    "for i in col:\n",
    "    if x[i].corr(y) == 1:\n",
    "        df.drop(i,axis = 1,inplace=True)\n",
    "    else:\n",
    "        continue\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "SC = StandardScaler()\n",
    "fea = []\n",
    "tar = dict1[\"design_state_data\"][\"target\"][\"target\"]\n",
    "for i in col:\n",
    "    if ((df[i]).dtype != \"object\"):\n",
    "        fea.append(i)\n",
    "    else:\n",
    "        continue\n",
    "df[tar] = SC.fit_transform(df[[tar]])\n",
    "x[fea] = SC.fit_transform(x[fea])\n",
    "y = df[tar]\n",
    "no_of_columns = len(x.columns)\n",
    "if no_of_columns > int(dict1[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"]):\n",
    "    pca = PCA(n_components= int(dict1[\"design_state_data\"][\"feature_reduction\"][\"num_of_features_to_keep\"]))\n",
    "    x[fea] = pca.fit_transform(x[fea])\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.25,random_state=2)\n",
    "from sklearn import tree,linear_model,ensemble,neighbors,svm\n",
    "from sklearn.metrics import r2_score,accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def regg(model):\n",
    "    model.fit(xtrain,ytrain)\n",
    "    pred = model.predict(xtest)\n",
    "    train = model.score(xtrain,ytrain)\n",
    "    test = model.score(xtest,ytest)\n",
    "    print(f\"train : {train}\\ntest : {test}\")\n",
    "    print(f\"r2_score : {r2_score(ytest,pred)}\")\n",
    "    print(\"-\"*100)\n",
    "def classification(model):\n",
    "    model.fit(xtrain,ytrain)\n",
    "    pred = model.predict(xtest)\n",
    "    train = model.score(xtrain,ytrain)\n",
    "    test = model.score(xtest,ytest)\n",
    "    print(f\"train : {train}\\ntest : {test}\")\n",
    "    print(f\"accuracy_score : {accuracy_score(ytest,pred)}\")\n",
    "    print(f\"classification_report :\\n {classification_report(ytest,pred)}\")\n",
    "    print(f\"confusion_matrix :\\n {confusion_matrix(ytest,pred)}\")\n",
    "    print(\"-\"*100)\n",
    "algorithms = []\n",
    "for i in dict1[\"design_state_data\"][\"algorithms\"].keys():\n",
    "    algorithms.append(i)\n",
    "if (dict1[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"Regression\") or (dict1[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"regression\"):\n",
    "    if \"LinearRegression\" in algorithms:\n",
    "        print(\"LinearRegression\")\n",
    "        regg(linear_model.LinearRegression())\n",
    "    if \"DecisionTreeRegressor\" in algorithms:\n",
    "        print(\"DecisionTreeRegressor\")\n",
    "        regg(tree.DecisionTreeRegressor())\n",
    "    if \"RandomForestRegressor\" in algorithms:\n",
    "        print(\"RandomForestRegressor\")\n",
    "        regg(ensemble.RandomForestRegressor())\n",
    "    if \"GBTRegressor\" in algorithms:\n",
    "        print(\"GBTRegressor\")\n",
    "        regg(ensemble.GradientBoostingRegressor())\n",
    "    if \"neural_network\" in algorithms:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential \n",
    "        from tensorflow.keras.layers import Dense\n",
    "    ann = Sequential()\n",
    "    hidden_layer = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"hidden_layer_sizes\"]\n",
    "    hidden = choice(hidden_layer)\n",
    "    ann.add(Dense(units = hidden,activation = \"relu\"))\n",
    "    ann.add(Dense(units = 1))\n",
    "    solver = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"solver\"]\n",
    "    ann.compile(optimizer = solver,loss = \"mse\")\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    earlystop = EarlyStopping(monitor = \"val_loss\",mode = \"min\",verbose = 1,patience = 25)\n",
    "    ann.fit(xtrain,ytrain,epochs = 500,validation_data = (xtest,ytest),callbacks = [earlystop])\n",
    "    ypred = ann.predict(xtest)\n",
    "    print(\"-\"*100)\n",
    "    print(\"r2_score:\",r2_score(ytest,ypred))\n",
    "if (dict1[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"Classification\") or (dict1[\"design_state_data\"][\"target\"][\"prediction_type\"] == \"classification\"):\n",
    "    if \"LogisticRegression\" in algorithms:\n",
    "        print(\"LogisticRegression\")\n",
    "        regg(linear_model.LogisticRegression())\n",
    "    if \"DecisionTreeClassifier\" in algorithms:\n",
    "        print(\"DecisionTreeClassifier\")\n",
    "        regg(tree.DecisionTreeClassifier())\n",
    "    if \"RandomForestClassifier\" in algorithms:\n",
    "        print(\"RandomForestClassifier\")\n",
    "        regg(ensemble.RandomForestClassifier())\n",
    "    if \"GBTClassifier\" in algorithms:\n",
    "        print(\"GBTClassifier\")\n",
    "        regg(ensemble.GradientBoostingClassifier())\n",
    "    if \"neural_network\" in algorithms:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import Sequential \n",
    "        from tensorflow.keras.layers import Dense\n",
    "    g = []\n",
    "    for i in df[\"species\"].unique():\n",
    "        g.append(i)\n",
    "    no_of_catogories = len(g)\n",
    "    if no_of_catogories > 2:\n",
    "        ann = Sequential()\n",
    "        hidden_layer = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"hidden_layer_sizes\"]\n",
    "        hidden = choice(hidden_layer)\n",
    "        ann.add(Dense(units = hidden,activation = \"relu\"))\n",
    "        ann.add(Dense(units = no_of_catogories,activation=\"softmax\"))\n",
    "        solver = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"solver\"]\n",
    "        ann.compile(optimizer = solver,loss = \"sparse_categorical_crossentropy\")\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        earlystop = EarlyStopping(monitor = \"val_loss\",mode = \"min\",verbose = 1,patience = 25)\n",
    "        ann.fit(xtrain,ytrain,epochs = 500,validation_data = (xtest,ytest),callbacks = [earlystop])\n",
    "        ypred = ann.predict(xtest)\n",
    "        print(\"-\"*100)\n",
    "        print(\"r2_score:\",r2_score(ytest,ypred))\n",
    "    else:\n",
    "        ann = Sequential()\n",
    "        hidden_layer = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"hidden_layer_sizes\"]\n",
    "        hidden = choice(hidden_layer)\n",
    "        ann.add(Dense(units = hidden,activation = \"relu\"))\n",
    "        ann.add(Dense(units = 1,activation=\"sigmoid\"))\n",
    "        solver = dict1[\"design_state_data\"][\"algorithms\"][\"neural_network\"][\"solver\"]\n",
    "        ann.compile(optimizer = solver,loss = \"binary_crossentropy\")\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        earlystop = EarlyStopping(monitor = \"val_loss\",mode = \"min\",verbose = 1,patience = 25)\n",
    "        ann.fit(xtrain,ytrain,epochs = 500,validation_data = (xtest,ytest),callbacks = [earlystop])\n",
    "        ypred = ann.predict(xtest)\n",
    "        print(\"-\"*100)\n",
    "        print(\"r2_score:\",r2_score(ytest,ypred))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
